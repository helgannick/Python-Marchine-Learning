{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RFr2TSVSpfkI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # Carrega a oarte de validação\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Criar um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "YN5QaJayqyK2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "p1H2WgpItuiu",
        "outputId": "6285868d-d306-4f03-99a1-6eef1a8bbeb6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape)#para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape)#para verificar as dimesões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7h5xiSRvSc9",
        "outputId": "1dccbee6-ca3f-48ed-a108-5d2ffb5fd060"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  class Modelo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Modelo, self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, 128) # canada de entrada, 784 neurônios que se ligam a 128\n",
        "        self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neurônios que ligam a 64\n",
        "        self.linear3 = nn.Linear(64, 10) # camada interna 2, 64 neurônios que se ligam a 10\n",
        "        # para a camada de saida não e necessário definir nada pois só precisamos pega o output da camada interna 2\n",
        "\n",
        "    def forward(self,X):\n",
        "      X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a camada interna 1\n",
        "      X = F.relu(self.linear2(X)) #função de ativação da camada interna 1 para a acamada interna 2\n",
        "      X = self.linear3(X) # função de ativação da camada interna 2 para a acamada de saída. nesse caso f(x) = x\n",
        "      return F.log_softmax(X, dim=1) #dados ultilizados para calcular a perda\n",
        "      "
      ],
      "metadata": {
        "id": "Yzq9d6HrwIq0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a política de atualização dos pesos e da bias\n",
        "    inicio = time() # timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "    criterio = nn.NLLoss() # definindo o criterio para calcular a perda\n",
        "    EPOCHS = 0 # numero de apachs que o algoritmo rodará\n",
        "    modelo.train() #ativando o mode de treinamento do modelo\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        perda_acumulada = 0 # inicializção da perda acumulada da epoch em questão\n",
        "\n",
        "        for imagens, etiquetas in trainloader:\n",
        "\n",
        "          imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para 'vetores' de 28*28 casas para ficarem comátíveis \n",
        "          otimizador.zero_grad() #zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "          output = modelo(imagens.to(device)) #colocando os dados no modelo\n",
        "\n",
        "          perda_instantanea.backward() # back propagation a partir da perda\n",
        "\n",
        "          otimizador.step() # atualizando os pesos e a bias\n",
        "\n",
        "          perda_acumulada += perda_instantanea.item() #atualização da perda acumulada\n",
        "\n",
        "\n",
        "        else:\n",
        "            print('Epoch {} - Perda resultante: {}'.format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "            print('\\nTempo de treino (em minutos) =',(time()-inicio)/60)\n",
        "\n"
      ],
      "metadata": {
        "id": "ITDAtN880zuV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "    conta_corretas, conta_todas = 0, 0\n",
        "    for imagens, etiquetas in valloader:\n",
        "      for i in range(len(etiquetas)):\n",
        "        img = imagens[i].view(1,784)\n",
        "        #desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "        with torch.no_grad():\n",
        "          logps = modelo(img.to(device)) # output do modelo em escala lagaritmica\n",
        "\n",
        "\n",
        "        ps = torch.exp(logps) #converte output para escala normal(lembrando que é um tensor)\n",
        "        probab = list(ps.cpu().numpy()[0])\n",
        "        etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
        "        etiqueta_certa = etiquetas.numpy()[i]\n",
        "        if(etiqueta_certa == etiqueta_pred): #compara a previsão com o valor correto\n",
        "          conta_corretas += 1\n",
        "        conta_todas += 1\n",
        "\n",
        "    \n",
        "    print('Total de imagens testadas =', conta_todas)\n",
        "    print('\\nPrecisão do modelo = {}%'.format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "5umUocZ_7Nst"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo() \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU31G2DZ9zzl",
        "outputId": "7fe45437-f3af-4404-9489-f25799e6d489"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCxWXi7T-dk1"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}